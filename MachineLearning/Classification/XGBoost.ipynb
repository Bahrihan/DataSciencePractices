{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04541dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c27618",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "df = diabetes.copy()\n",
    "y = df['Outcome']\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5482811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663c2bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d106a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        XGBClassifier\n",
      "\u001b[1;31mString form:\u001b[0m\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "           colsample_bylevel=None <...> , multi_strategy=None, n_estimators=None,\n",
      "           n_jobs=None, num_parallel_tree=None, ...)\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\users\\lenovo\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "Implementation of the scikit-learn API for XGBoost classification.\n",
      "See :doc:`/python/sklearn_estimator` for more information.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "    n_estimators : Optional[int]\n",
      "        Number of boosting rounds.\n",
      "\n",
      "    max_depth :  typing.Optional[int]\n",
      "\n",
      "        Maximum tree depth for base learners.\n",
      "\n",
      "    max_leaves : typing.Optional[int]\n",
      "\n",
      "        Maximum number of leaves; 0 indicates no limit.\n",
      "\n",
      "    max_bin : typing.Optional[int]\n",
      "\n",
      "        If using histogram-based algorithm, maximum number of bins per feature\n",
      "\n",
      "    grow_policy : typing.Optional[str]\n",
      "\n",
      "        Tree growing policy.\n",
      "\n",
      "        - depthwise: Favors splitting at nodes closest to the node,\n",
      "        - lossguide: Favors splitting at nodes with highest loss change.\n",
      "\n",
      "    learning_rate : typing.Optional[float]\n",
      "\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "\n",
      "    verbosity : typing.Optional[int]\n",
      "\n",
      "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "\n",
      "    objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "\n",
      "        Specify the learning task and the corresponding learning objective or a custom\n",
      "        objective function to be used.\n",
      "\n",
      "        For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
      "        :ref:`custom-obj-metric` for more information, along with the end note for\n",
      "        function signatures.\n",
      "\n",
      "    booster: typing.Optional[str]\n",
      "\n",
      "        Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
      "\n",
      "    tree_method : typing.Optional[str]\n",
      "\n",
      "        Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
      "        default, XGBoost will choose the most conservative option available.  It's\n",
      "        recommended to study this option from the parameters document :doc:`tree method\n",
      "        </treemethod>`\n",
      "\n",
      "    n_jobs : typing.Optional[int]\n",
      "\n",
      "        Number of parallel threads used to run xgboost.  When used with other\n",
      "        Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
      "        parallelize and balance the threads.  Creating thread contention will\n",
      "        significantly slow down both algorithms.\n",
      "\n",
      "    gamma : typing.Optional[float]\n",
      "\n",
      "        (min_split_loss) Minimum loss reduction required to make a further partition on\n",
      "        a leaf node of the tree.\n",
      "\n",
      "    min_child_weight : typing.Optional[float]\n",
      "\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "\n",
      "    max_delta_step : typing.Optional[float]\n",
      "\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "\n",
      "    subsample : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of the training instance.\n",
      "\n",
      "    sampling_method : typing.Optional[str]\n",
      "\n",
      "        Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
      "\n",
      "        - ``uniform``: Select random training instances uniformly.\n",
      "        - ``gradient_based``: Select random training instances with higher probability\n",
      "            when the gradient and hessian are larger. (cf. CatBoost)\n",
      "\n",
      "    colsample_bytree : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "\n",
      "    colsample_bylevel : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of columns for each level.\n",
      "\n",
      "    colsample_bynode : typing.Optional[float]\n",
      "\n",
      "        Subsample ratio of columns for each split.\n",
      "\n",
      "    reg_alpha : typing.Optional[float]\n",
      "\n",
      "        L1 regularization term on weights (xgb's alpha).\n",
      "\n",
      "    reg_lambda : typing.Optional[float]\n",
      "\n",
      "        L2 regularization term on weights (xgb's lambda).\n",
      "\n",
      "    scale_pos_weight : typing.Optional[float]\n",
      "        Balancing of positive and negative weights.\n",
      "\n",
      "    base_score : typing.Optional[float]\n",
      "\n",
      "        The initial prediction score of all instances, global bias.\n",
      "\n",
      "    random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
      "\n",
      "        Random number seed.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           Using gblinear booster with shotgun updater is nondeterministic as\n",
      "           it uses Hogwild algorithm.\n",
      "\n",
      "    missing : float\n",
      "\n",
      "        Value in the data which needs to be present as a missing value. Default to\n",
      "        :py:data:`numpy.nan`.\n",
      "\n",
      "    num_parallel_tree: typing.Optional[int]\n",
      "\n",
      "        Used for boosting random forest.\n",
      "\n",
      "    monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
      "\n",
      "        Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
      "        for more information.\n",
      "\n",
      "    interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
      "\n",
      "        Constraints for interaction representing permitted interactions.  The\n",
      "        constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
      "        3, 4]]``, where each inner list is a group of indices of features that are\n",
      "        allowed to interact with each other.  See :doc:`tutorial\n",
      "        </tutorials/feature_interaction_constraint>` for more information\n",
      "\n",
      "    importance_type: typing.Optional[str]\n",
      "\n",
      "        The feature importance type for the feature_importances\\_ property:\n",
      "\n",
      "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "          \"total_cover\".\n",
      "        * For linear model, only \"weight\" is defined and it's the normalized\n",
      "          coefficients without bias.\n",
      "\n",
      "    device : typing.Optional[str]\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
      "\n",
      "    validate_parameters : typing.Optional[bool]\n",
      "\n",
      "        Give warnings for unknown parameter.\n",
      "\n",
      "    enable_categorical : bool\n",
      "\n",
      "        See the same parameter of :py:class:`DMatrix` for details.\n",
      "\n",
      "    feature_types : typing.Optional[typing.Sequence[str]]\n",
      "\n",
      "        .. versionadded:: 1.7.0\n",
      "\n",
      "        Used for specifying feature types without constructing a dataframe. See\n",
      "        :py:class:`DMatrix` for details.\n",
      "\n",
      "    feature_weights : Optional[ArrayLike]\n",
      "\n",
      "        Weight for each feature, defines the probability of each feature being selected\n",
      "        when colsample is being used.  All values must be greater than 0, otherwise a\n",
      "        `ValueError` is thrown.\n",
      "\n",
      "    max_cat_to_onehot : Optional[int]\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        .. note:: This parameter is experimental\n",
      "\n",
      "        A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
      "        for categorical data.  When number of categories is lesser than the threshold\n",
      "        then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
      "        into children nodes. Also, `enable_categorical` needs to be set to have\n",
      "        categorical feature support. See :doc:`Categorical Data\n",
      "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      "\n",
      "    max_cat_threshold : typing.Optional[int]\n",
      "\n",
      "        .. versionadded:: 1.7.0\n",
      "\n",
      "        .. note:: This parameter is experimental\n",
      "\n",
      "        Maximum number of categories considered for each split. Used only by\n",
      "        partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
      "        needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
      "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      "\n",
      "    multi_strategy : typing.Optional[str]\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        .. note:: This parameter is working-in-progress.\n",
      "\n",
      "        The strategy used for training multi-target models, including multi-target\n",
      "        regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
      "        more information.\n",
      "\n",
      "        - ``one_output_per_tree``: One model for each target.\n",
      "        - ``multi_output_tree``:  Use multi-target trees.\n",
      "\n",
      "    eval_metric : typing.Union[str, typing.List[str], typing.Callable, NoneType]\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        Metric used for monitoring the training result and early stopping.  It can be a\n",
      "        string or list of strings as names of predefined metric in XGBoost (See\n",
      "        :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
      "        other user defined metric that looks like `sklearn.metrics`.\n",
      "\n",
      "        If custom objective is also provided, then custom metric should implement the\n",
      "        corresponding reverse link function.\n",
      "\n",
      "        Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
      "        object is provided, it's assumed to be a cost function and by default XGBoost\n",
      "        will minimize the result during early stopping.\n",
      "\n",
      "        For advanced usage on Early stopping like directly choosing to maximize instead\n",
      "        of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
      "\n",
      "        See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
      "        information.\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            from sklearn.datasets import load_diabetes\n",
      "            from sklearn.metrics import mean_absolute_error\n",
      "            X, y = load_diabetes(return_X_y=True)\n",
      "            reg = xgb.XGBRegressor(\n",
      "                tree_method=\"hist\",\n",
      "                eval_metric=mean_absolute_error,\n",
      "            )\n",
      "            reg.fit(X, y, eval_set=[(X, y)])\n",
      "\n",
      "    early_stopping_rounds : typing.Optional[int]\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        - Activates early stopping. Validation metric needs to improve at least once in\n",
      "          every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
      "          least one item in **eval_set** in :py:meth:`fit`.\n",
      "\n",
      "        - If early stopping occurs, the model will have two additional attributes:\n",
      "          :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
      "          :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
      "          number of trees during inference. If users want to access the full model\n",
      "          (including trees built after early stopping), they can specify the\n",
      "          `iteration_range` in these inference methods. In addition, other utilities\n",
      "          like model plotting can also use the entire model.\n",
      "\n",
      "        - If you prefer to discard the trees after `best_iteration`, consider using the\n",
      "          callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
      "\n",
      "        - If there's more than one item in **eval_set**, the last entry will be used for\n",
      "          early stopping.  If there's more than one metric in **eval_metric**, the last\n",
      "          metric will be used for early stopping.\n",
      "\n",
      "    callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
      "\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           States in callback are not preserved during training, which means callback\n",
      "           objects can not be reused for multiple training sessions without\n",
      "           reinitialization or deepcopy.\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            for params in parameters_grid:\n",
      "                # be sure to (re)initialize the callbacks before each run\n",
      "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      "                reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
      "                reg.fit(X, y)\n",
      "\n",
      "    kwargs : typing.Optional[typing.Any]\n",
      "\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
      "        can be found :doc:`here </parameter>`.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "        dict simultaneously will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "            that parameters passed via this argument will interact properly\n",
      "            with scikit-learn.\n",
      "\n",
      "        .. note::  Custom objective function\n",
      "\n",
      "            A custom objective function can be provided for the ``objective``\n",
      "            parameter. In this case, it should have the signature ``objective(y_true,\n",
      "            y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
      "            -> [grad, hess]``:\n",
      "\n",
      "            y_true: array_like of shape [n_samples]\n",
      "                The target values\n",
      "            y_pred: array_like of shape [n_samples]\n",
      "                The predicted values\n",
      "            sample_weight :\n",
      "                Optional sample weights.\n",
      "\n",
      "            grad: array_like of shape [n_samples]\n",
      "                The value of the gradient for each sample point.\n",
      "            hess: array_like of shape [n_samples]\n",
      "                The value of the second derivative for each sample point\n",
      "\n",
      "            Note that, if the custom objective produces negative values for\n",
      "            the Hessian, these will be clipped. If the objective is non-convex,\n",
      "            one might also consider using the expected Hessian (Fisher\n",
      "            information) instead."
     ]
    }
   ],
   "source": [
    "?xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227543d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "        'n_estimators': [100, 500, 1000, 2000],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5,6],\n",
    "        'learning_rate': [0.1,0.01,0.02,0.05]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf35064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb_cv = GridSearchCV(xgb, xgb_params, cv=10, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94be482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 192 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "xgb_cv_model = xgb_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29f37835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ad2500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned = XGBClassifier(n_estimators= xgb_cv_model.best_params_['n_estimators'],\n",
    "                          max_depth= xgb_cv_model.best_params_['max_depth'],\n",
    "                          learning_rate= xgb_cv_model.best_params_['learning_rate'],\n",
    "                          subsample= xgb_cv_model.best_params_['subsample']).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6540878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575757575757576"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tuned = xgb_tuned.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bad905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
